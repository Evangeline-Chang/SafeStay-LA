{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performed LDA Topic Modeling on Reviews\n",
    "2023-10-28<br>\n",
    "Evangeline Chang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/evangeline/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/evangeline/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/evangeline/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/evangeline/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/evangeline/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data/reviews_check.csv', index_col=0)\n",
    "reviews.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020.0</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021.0</td>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022.0</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  Years\n",
       "0  2017.0      8\n",
       "1  2018.0    242\n",
       "2  2019.0    268\n",
       "3  2020.0    350\n",
       "4  2021.0    382\n",
       "5  2022.0    381\n",
       "6  2023.0    163"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at when the reviews were posted\n",
    "def get_year(in_file):\n",
    "    for index, row in in_file.iterrows():\n",
    "        date_str = row['date']\n",
    "        date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        year = date_obj.year\n",
    "\n",
    "        in_file.at[index, 'Year'] = int(year)\n",
    "\n",
    "get_year(reviews)\n",
    "\n",
    "review_years = reviews.groupby(['Year']).size().reset_index(name='Years')\n",
    "review_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings = pd.read_csv('data/listings_check.csv', index_col=0)\n",
    "\n",
    "# check if there are reviews that do not match with the listings from the listings file\n",
    "unique_ids = listings['id'].unique()\n",
    "len(reviews[reviews['listing_id'].isin(unique_ids)]) == len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the reviews to after 2020\n",
    "review_after2020 = reviews[reviews['Year'] >= 2020]\n",
    "review_after2020 = review_after2020.drop(columns=['reviewer_id', 'reviewer_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_after2020['comments'] = review_after2020['comments'].str.strip()\n",
    "review_after2020 = review_after2020[review_after2020['comments'] != '']\n",
    "\n",
    "# drop na\n",
    "review_after2020.dropna(inplace=True)\n",
    "\n",
    "# remove the rows with less than 5 characters\n",
    "mask = review_after2020['comments'].str.len() >= 5\n",
    "review_after2020 = review_after2020[mask]\n",
    "\n",
    "review_after2020.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove <br/>\n",
    "def empty_lines(text):\n",
    "    return text.replace('<br/>', '')\n",
    "    \n",
    "review_after2020['comments'] = review_after2020['comments'].apply(empty_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 1270/1270 [00:00<00:00, 20828.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# remove host names from the comments\n",
    "def english_only(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[^A-Za-z ]+', '', text)\n",
    "    return text\n",
    "\n",
    "listings['host_name'] = listings['host_name'].apply(english_only)\n",
    "listings['host_name'] = listings['host_name'].replace('Chas', 'Charles')\n",
    "\n",
    "merged_df = review_after2020.merge(listings[['id', 'host_name']], left_on='listing_id', right_on='id', how='left')\n",
    "\n",
    "tqdm.pandas()\n",
    "for index, row in tqdm(merged_df.iterrows(), total=len(merged_df), desc=\"Processing rows\"):\n",
    "    name = row['host_name']\n",
    "    if name != '':\n",
    "        comment = row['comments']\n",
    "        comment = re.sub(r'\\b' + re.escape(name) + r'\\b', '', comment, flags=re.IGNORECASE)\n",
    "        merged_df.at[index, 'comments'] = comment\n",
    "\n",
    "review_after2020 = merged_df.drop(columns=['id_x', 'id_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1270/1270 [00:00<00:00, 96168.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# remove names and non-English characters from comments\n",
    "tqdm.pandas()\n",
    "review_after2020['comments'] = review_after2020['comments'].progress_apply(english_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_after2020['comments'] = review_after2020['comments'].str.strip()\n",
    "review_after2020 = review_after2020[review_after2020['comments'] != '']\n",
    "\n",
    "mask = review_after2020['comments'].str.len() >= 5\n",
    "review_after2020 = review_after2020[mask]\n",
    "\n",
    "review_after2020.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39471761</td>\n",
       "      <td>613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22215734</td>\n",
       "      <td>579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23395506</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id  review_count\n",
       "2    39471761           613\n",
       "0    22215734           579\n",
       "1    23395506            73"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see each listing has how many reviews\n",
    "review_counts = review_after2020.groupby(['listing_id']).size().reset_index(name='review_count')\n",
    "review_counts.sort_values(by=['review_count'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# filter those listings with more than 100 reviews to perform LDA model\n",
    "review_counts_100 = review_counts[review_counts['review_count'] >= 100]\n",
    "print(len(review_counts_100))\n",
    "count_100_ids = review_counts_100['listing_id'].unique()\n",
    "review_after2020_100 = review_after2020[review_after2020['listing_id'].isin(count_100_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 1192/1192 [00:00<00:00, 26456.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# for those listings with host names more than one word, e.g. Jeff and Shelley, use another method to remove them\n",
    "review_after2020_100['host_name'] = review_after2020_100['host_name'].apply(lambda s: s.replace(' and', '').replace(' And', ''))\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def remove_host(reviews):\n",
    "    for index, row in tqdm(reviews.iterrows(), total=len(reviews), desc=\"Processing rows\"):\n",
    "        names = row['host_name']\n",
    "        comment = row['comments']\n",
    "        if ' ' in names:\n",
    "            names_split = names.split(' ')\n",
    "            names_split = [item for item in names_split if len(item) > 1]\n",
    "            if len(names_split) > 1:\n",
    "                for name in names_split:\n",
    "                    comment = re.sub(r'\\b' + re.escape(name) + r'\\b', '', comment, flags=re.IGNORECASE)\n",
    "                    reviews.at[index, 'comments'] = comment \n",
    "\n",
    "remove_host(review_after2020_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1192/1192 [00:01<00:00, 1041.05it/s]\n"
     ]
    }
   ],
   "source": [
    "stop_words = list(set(nltk.corpus.stopwords.words('english')))\n",
    "\n",
    "# these words basically don't mean anything, remove them\n",
    "more = ['stay', 'place', 'would', 'Airbnb', 'airbnb', 'youre', 'time', 'area', 'street', 'bnb', 'BNB', 'B&B', 'b&b',\n",
    "        'host', 'wife', 'husband', 'absolutely', 'really', 'proximity', 'probably', 'definitely']\n",
    "stop_words.extend(more)\n",
    "\n",
    "def clean_text(headline):\n",
    "    if isinstance(headline, str):\n",
    "        le = WordNetLemmatizer()\n",
    "        word_tokens = word_tokenize(headline)\n",
    "        tokens = [le.lemmatize(w) for w in word_tokens if w not in stop_words and len(w) > 3]\n",
    "        cleaned_text = \" \".join(tokens)\n",
    "        return cleaned_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "tqdm.pandas()\n",
    "review_after2020_100['cleaned_text'] = review_after2020_100['comments'].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(review):\n",
    "    # Define stop words and create a TfidfVectorizer\n",
    "    vect = TfidfVectorizer(stop_words=stop_words, max_features=1000)\n",
    "    vect_text = vect.fit_transform(review['cleaned_text'])\n",
    "\n",
    "    # Create a Latent Dirichlet Allocation model\n",
    "    lda_model = LatentDirichletAllocation(n_components=1, learning_method='online', random_state=42, max_iter=1)\n",
    "    lda_top = lda_model.fit_transform(vect_text)\n",
    "\n",
    "    # Get the vocabulary from the TfidfVectorizer\n",
    "    vocab = vect.get_feature_names_out()\n",
    "\n",
    "    for i, comp in enumerate(lda_model.components_):\n",
    "        vocab_comp = list(zip(vocab, comp))\n",
    "        sorted_words = sorted(vocab_comp, key=lambda x: x[1], reverse=True)[:10]\n",
    "        topic_words_list = [t[0] for t in sorted_words] \n",
    "\n",
    "    return topic_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 11.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# run lda model on the listings with more than 100 reviews\n",
    "unique_ids = list(review_counts_100['listing_id'].unique())\n",
    "data = {'id': unique_ids, 'topic': [None] * len(unique_ids)}  \n",
    "listing_topic = pd.DataFrame(data)\n",
    "\n",
    "for id in tqdm(list(count_100_ids)):\n",
    "    topic_list = list(set(lda(review_after2020_100[review_after2020_100['listing_id'] == id])))\n",
    "    topic_string = ', '.join(w for w in topic_list)\n",
    "    listing_topic.loc[listing_topic['id'] == id, 'topic'] = topic_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top five topics of each listing\n",
    "def top5(text):\n",
    "    if text != None:\n",
    "        split = text.split(', ')\n",
    "        five = split[0:5]\n",
    "        return five\n",
    "\n",
    "listing_topic['top5'] = listing_topic['topic'].apply(top5)\n",
    "\n",
    "listing_topic['first'] = listing_topic['top5'].apply(lambda x: x[0] if x else None)\n",
    "listing_topic['second'] = listing_topic['top5'].apply(lambda x: x[1] if x != None and len(x) > 1 else None)\n",
    "listing_topic['third'] = listing_topic['top5'].apply(lambda x: x[2] if x != None and len(x) > 2 else None)\n",
    "listing_topic['fourth'] = listing_topic['top5'].apply(lambda x: x[3] if x != None and len(x) > 3 else None)\n",
    "listing_topic['fifth'] = listing_topic['top5'].apply(lambda x: x[4] if x != None and len(x) > 4 else None)\n",
    "\n",
    "# add ', ' so that it looks better on Tableau\n",
    "listing_topic['first'] = listing_topic['first'].apply(lambda x: x + ', ' if x != None else None)\n",
    "listing_topic['second'] = listing_topic['second'].apply(lambda x: x + ', ' if x != None else None)\n",
    "listing_topic['third'] = listing_topic['third'].apply(lambda x: x + ', ' if x != None else None)\n",
    "listing_topic['fourth'] = listing_topic['fourth'].apply(lambda x: x + ', ' if x != None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add another column; if safe is one of the topics (no matter which one), mark it\n",
    "listing_topic['safe'] = ''\n",
    "listing_topic['safe'] = listing_topic.apply(lambda row: ', safe' if 'safe' in row['topic'] and 'safe' not in row['top5'] else None, axis=1)\n",
    "\n",
    "listing_topic = listing_topic.drop('top5', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join topic df with the original df\n",
    "listing_topic_joined = pd.merge(listings, listing_topic, on='id', how='left')\n",
    "\n",
    "# this column will be used on the tooltip of the map\n",
    "listing_topic_joined['topic_title'] = ''\n",
    "listing_topic_joined.loc[listing_topic_joined['topic'].notna(), 'topic_title'] = 'Top Topics of Comments: '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_topic_joined.to_csv('data/Airbnb_BasicInfo_check.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
